{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seNjRVzmE3Du"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tweepy schedule transformers torch requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 135725,
     "status": "ok",
     "timestamp": 1742186070233,
     "user": {
      "displayName": "Prathap Hm",
      "userId": "06877835029504073666"
     },
     "user_tz": -330
    },
    "id": "bcuh1g9xK-bE",
    "outputId": "0a994383-7412-4626-fc27-33c311f3e469"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "from typing import List, Dict, Optional, Set\n",
    "from transformers import pipeline, logging as transformers_logging\n",
    "import tweepy\n",
    "\n",
    "# Twitter API Credentials\n",
    "API_KEY = \"lgKfDigC7qbos2zUIQeCvfbph\"\n",
    "API_SECRET_KEY = \"NkkFO9wCVvsvv4m6GOssoXC0rCcE4qUav0rvVvM5xhWuJIkod2\"\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAALbPzwEAAAAA55b7v5kLBGPx1JyibiHmBQpmxoo%3Dhzw1o30lEajXtkuinTH1IAmjnIE4pILWy8e1wklNvWRA6N8j7f\"\n",
    "ACCESS_TOKEN = \"1674821745340211200-UQzdIVGRCTSt7D5411vpxV9ApP9msO\"\n",
    "ACCESS_TOKEN_SECRET = \"EjwT4klBT9nelP3aDonZz6hcruZ6IciO987TpRnnPFkif\"\n",
    "CLIENT_ID = \"UFAwM1M1aWs2TVgyZ2F6R3Z4RzU6MTpjaQ\"\n",
    "CLIENT_SECRET = \"yy0c-4hcra6OJz0RodTF60gWAg2Uj802bWKUl3XErsUzJUb06n\"\n",
    "\n",
    "# Suppress warnings and non-critical logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TweetGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"distilgpt2\",  # Lightweight, fast model\n",
    "        topics_file: str = \"topics.json\",  # Source of tweet prompts\n",
    "        max_tweet_length: int = 280,  # X's max tweet length\n",
    "        cache_dir: Optional[str] = None  # Unused cache option\n",
    "    ):\n",
    "        \"\"\"Initialize the TweetGenerator with model and topics.\"\"\"\n",
    "        self.topics_file = topics_file\n",
    "        self.max_tweet_length = max_tweet_length\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.topics = self._load_static_topics()  # Load or create topics\n",
    "        self.used_topics: Set[int] = set()  # Track used topics\n",
    "        self._setup_generator()  # Setup AI model\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    def _setup_generator(self):\n",
    "        \"\"\"Initialize the distilgpt2 model pipeline.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading {self.model_name} model...\")\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                self.generator = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "                )\n",
    "            logger.info(\"Model initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _load_static_topics(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Load or create default topics from/to topics.json.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.topics_file):\n",
    "                sample_topics = [\n",
    "                    {\"topic\": \"Future of AI\", \"prompt\": \"AI is evolving beyond automation, unlocking\"},\n",
    "                    {\"topic\": \"Immersive Gaming\", \"prompt\": \"Gaming is shifting towards hyper-realistic worlds with\"},\n",
    "                    {\"topic\": \"Decentralized Finance\", \"prompt\": \"DeFi is transforming traditional banking by\"},\n",
    "                    {\"topic\": \"Code Optimization\", \"prompt\": \"Writing efficient code isn't just about speed, it's about\"},\n",
    "                    {\"topic\": \"Next-Gen Cybersecurity\", \"prompt\": \"Tomorrow's digital threats demand proactive security like\"},\n",
    "                    {\"topic\": \"The Rise of Hybrid Work\", \"prompt\": \"Blending remote and in-office work successfully depends on\"},\n",
    "                    {\"topic\": \"AI in Marketing\", \"prompt\": \"AI-driven marketing isn't just about ads, it's about\"},\n",
    "                    {\"topic\": \"The App Revolution\", \"prompt\": \"Smart apps are becoming personal assistants by\"}\n",
    "                ]\n",
    "                dir_path = os.path.dirname(os.path.abspath(self.topics_file))\n",
    "                if dir_path and not os.path.exists(dir_path):\n",
    "                    os.makedirs(dir_path, exist_ok=True)\n",
    "                with open(self.topics_file, 'w') as f:\n",
    "                    json.dump(sample_topics, f, indent=2)\n",
    "                logger.info(f\"Created new topics file with {len(sample_topics)} topics\")\n",
    "            with open(self.topics_file, 'r') as f:\n",
    "                topics = json.load(f)\n",
    "            logger.info(f\"Loaded {len(topics)} topics from {self.topics_file}\")\n",
    "            return topics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load static topics: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _get_next_topic(self) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Retrieve a random unused topic, reset if all used.\"\"\"\n",
    "        if not self.topics:\n",
    "            logger.error(\"No topics available\")\n",
    "            return None\n",
    "        if len(self.used_topics) >= len(self.topics):\n",
    "            logger.info(\"All topics have been used. Resetting.\")\n",
    "            self.used_topics.clear()\n",
    "        available_topics = [i for i in range(len(self.topics)) if i not in self.used_topics]\n",
    "        if available_topics:\n",
    "            topic_index = random.choice(available_topics)\n",
    "            self.used_topics.add(topic_index)\n",
    "            return self.topics[topic_index]\n",
    "        return None\n",
    "\n",
    "    def _clean_tweet(self, text: str) -> str:\n",
    "        \"\"\"Clean text to a single, concise sentence under 280 chars.\"\"\"\n",
    "        text = text.strip()  # Remove extra whitespace\n",
    "        sentences = text.split('.')  # Split into sentences\n",
    "        if len(sentences) > 1:  # Take only first sentence\n",
    "            text = sentences[0].strip() + '.'\n",
    "        text = ''.join(c for c in text if c.isalnum() or c in ' .,!?')  # Keep valid chars\n",
    "        if text and text[0].islower():  # Capitalize first letter\n",
    "            text = text[0].upper() + text[1:]\n",
    "        if text and not text[-1] in ['.', '!', '?']:  # Add ending punctuation\n",
    "            text += '.'\n",
    "        if len(text) > self.max_tweet_length:  # Truncate if too long\n",
    "            text = text[:self.max_tweet_length-1] + '.'\n",
    "        return text\n",
    "\n",
    "    def _generate_tweet_content(self, topic: Dict[str, str]) -> Optional[str]:\n",
    "        \"\"\"Generate a clean, concise tweet from a topic prompt.\"\"\"\n",
    "        try:\n",
    "            prompt = topic['prompt']  # Use prompt as base\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                outputs = self.generator(\n",
    "                    prompt,\n",
    "                    max_new_tokens=30,  # Reduced for conciseness (was 50)\n",
    "                    temperature=0.7,  # Lowered for coherence (was 0.85)\n",
    "                    top_p=0.9,  # Tighter for focus (was 0.92)\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=50256\n",
    "                )\n",
    "            generated_text = outputs[0]['generated_text']  # Get raw output\n",
    "            tweet = self._clean_tweet(generated_text)  # Clean to desired format\n",
    "            logger.info(f\"Generated tweet content: {tweet} ({len(tweet)} characters)\")\n",
    "            return tweet\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate tweet content: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def generate_tweet(self) -> Optional[str]:\n",
    "        \"\"\"Generate a single tweet from a random topic.\"\"\"\n",
    "        topic = self._get_next_topic()\n",
    "        if not topic:\n",
    "            logger.warning(\"No topic available\")\n",
    "            return None\n",
    "        tweet_content = self._generate_tweet_content(topic)\n",
    "        if not tweet_content:\n",
    "            logger.warning(\"Failed to generate tweet content\")\n",
    "            return None\n",
    "        logger.info(f\"Generated tweet ({len(tweet_content)} chars): {tweet_content}\")\n",
    "        return tweet_content\n",
    "\n",
    "    def generate_engaging_tweet(self) -> Optional[str]:\n",
    "        \"\"\"Generate an engaging tweet, picking the best from 3 attempts.\"\"\"\n",
    "        best_tweet = None\n",
    "        best_length = 0\n",
    "        for _ in range(3):  # Try 3 times for quality\n",
    "            topic = self._get_next_topic()\n",
    "            if not topic:\n",
    "                continue\n",
    "            tweet = self._generate_tweet_content(topic)\n",
    "            if not tweet:\n",
    "                continue\n",
    "            tweet_length = len(tweet)\n",
    "            if (best_tweet is None) or (60 <= tweet_length <= 200 and tweet_length > best_length):  # Adjusted range for conciseness\n",
    "                best_tweet = tweet\n",
    "                best_length = tweet_length\n",
    "        if best_tweet:\n",
    "            logger.info(f\"Selected best tweet: {best_tweet}\")\n",
    "            return best_tweet\n",
    "        else:\n",
    "            return self.generate_tweet()  # Fallback if no good tweet\n",
    "\n",
    "    def generate_tweets_batch(self, count: int = 3) -> List[str]:\n",
    "        \"\"\"Generate multiple tweets (unused here).\"\"\"\n",
    "        tweets = []\n",
    "        for _ in range(count):\n",
    "            tweet = self.generate_tweet()\n",
    "            if tweet:\n",
    "                tweets.append(tweet)\n",
    "        return tweets\n",
    "\n",
    "    def add_topic(self, topic: str, prompt: str) -> bool:\n",
    "        \"\"\"Add a new topic to the file for future use.\"\"\"\n",
    "        try:\n",
    "            new_topic = {\"topic\": topic, \"prompt\": prompt}\n",
    "            self.topics.append(new_topic)\n",
    "            with open(self.topics_file, 'w') as f:\n",
    "                json.dump(self.topics, f, indent=2)\n",
    "            logger.info(f\"Added new topic: {topic}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to add topic: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def post_tweet(client, tweet):\n",
    "    \"\"\"Post a tweet using the X v2 API.\"\"\"\n",
    "    try:\n",
    "        response = client.create_tweet(text=tweet)\n",
    "        logger.info(f\"Tweet posted successfully with ID: {response.data['id']}\")\n",
    "        print(f\"\\nSuccessfully posted tweet: {tweet} (Tweet ID: {response.data['id']})\")\n",
    "    except tweepy.TweepyException as e:\n",
    "        logger.error(f\"Failed to post tweet: {str(e)}\")\n",
    "        print(f\"\\nFailed to post tweet: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate and post two tweets with a 2-minute interval.\"\"\"\n",
    "    print(\"\\n Initializing Tweet Generator...\")\n",
    "    generator = TweetGenerator()\n",
    "\n",
    "    client = tweepy.Client(\n",
    "        bearer_token=BEARER_TOKEN,\n",
    "        consumer_key=API_KEY,\n",
    "        consumer_secret=API_SECRET_KEY,\n",
    "        access_token=ACCESS_TOKEN,\n",
    "        access_token_secret=ACCESS_TOKEN_SECRET\n",
    "    )\n",
    "\n",
    "    # First tweet\n",
    "    print(\"\\n Generating first engaging tweet...\")\n",
    "    tweet1 = generator.generate_engaging_tweet()\n",
    "    if tweet1:\n",
    "        print(\"\\n FIRST ENGAGING TWEET \")\n",
    "        print(f\"{tweet1}\")\n",
    "        post_tweet(client, tweet1)\n",
    "    else:\n",
    "        print(\"Failed to generate first engaging tweet.\")\n",
    "\n",
    "    # Wait 2 minutes\n",
    "    print(\"\\nWaiting before posting the second tweet...\")\n",
    "    time.sleep(120)\n",
    "\n",
    "    # Second tweet\n",
    "    print(\"\\n Generating second engaging tweet...\")\n",
    "    tweet2 = generator.generate_engaging_tweet()\n",
    "    if tweet2:\n",
    "        print(\"\\n SECOND ENGAGING TWEET \")\n",
    "        print(f\"{tweet2}\")\n",
    "        post_tweet(client, tweet2)\n",
    "    else:\n",
    "        print(\"Failed to generate second engaging tweet.\")\n",
    "\n",
    "    logger.info(\"Two-tweet generation and posting process completed.\")\n",
    "    print(\"\\nProcess completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1742147724156,
     "user": {
      "displayName": "Prathap Hm",
      "userId": "06877835029504073666"
     },
     "user_tz": -330
    },
    "id": "J_niicMdDuIp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPw/xRVgUjBJg4ohfnGczYX",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
